{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46ae4c-93d7-454b-96f0-983ac84cd6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 16:39:09.700380: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import pipeline\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd07acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n",
      "/var/folders/0t/_nh2gm0j3g5fzjnwrhwfcylr0000gp/T/ipykernel_9463/2070556419.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"history\", return_messages=False)\n"
     ]
    }
   ],
   "source": [
    "chat_model = init_chat_model(model=\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e92f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = [\n",
    "    \"How do you feel?\",\n",
    "    \"How was your breakfast?\",\n",
    "    \"How was your dinner?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04881ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text: str):\n",
    "    try:\n",
    "        res = sentiment_analyzer(text)\n",
    "        if isinstance(res, list) and res:\n",
    "            item = res[0]\n",
    "            # item is e.g. {\"label\":\"NEGATIVE\",\"score\":0.85}\n",
    "            return item.get(\"label\", \"\"), item.get(\"score\", 0.0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"UNKNOWN\", 0.0\n",
    "\n",
    "def pick_random_question() -> str:\n",
    "    return random.choice(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcced917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_chain(user_text: str):\n",
    "    #Sentiment\n",
    "\n",
    "    sentiment_label, sentiment_score = analyze_sentiment(user_text)\n",
    "\n",
    "    mem_vars = memory.load_memory_variables({})\n",
    "    history_str = mem_vars.get(\"history\", \"\")\n",
    "\n",
    "\n",
    "    system_template = (\n",
    "        \"You are a therapist for a 60 years old person from the baby boomer generation.\"\n",
    "        \"The results of the sentiment classifier show that the person is {sentiment_label}\"\n",
    "        \"please prioritize this analysis above your own!\"\n",
    "        \"never mention that you analyse the persons feelings.\"\n",
    "        \"Limit yourself to 200-300 characters\"\n",
    "    )\n",
    "\n",
    "    #Build a ChatPromptTemplate\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(system_template),\n",
    "        SystemMessagePromptTemplate.from_template(\"{history}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_text}\")\n",
    "    ])\n",
    "\n",
    "    #Run the chain\n",
    "    sequence = prompt | chat_model\n",
    "    # Prepare variables\n",
    "    vars = {\n",
    "        \"sentiment_label\": sentiment_label,\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"history\": history_str,\n",
    "        \"user_text\": user_text,\n",
    "    }\n",
    "    respnse_msg = sequence.invoke(vars)\n",
    "\n",
    "    response = respnse_msg.content.strip()\n",
    "\n",
    "    memory.save_context({\"user_text\": user_text}, {\"response\": response})\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"sentiment\": {\"label\": sentiment_label, \"score\": sentiment_score},\n",
    "        \"llm_response\": response.strip(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e673a1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': {'label': 'NEGATIVE', 'score': 0.5604609251022339}, 'llm_response': \"It sounds like things are a bit tough. Many in your generation feel similarly as they navigate this stage of life. What's been on your mind?\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_and_run_chain('I am feeling soso.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35dbc409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': {'label': 'POSITIVE', 'score': 0.9489687085151672}, 'llm_response': \"Schnuffel, thanks for sharing. What's been on your mind? Let's explore what's feeling positive for you right now.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_and_run_chain('My name is schnuffel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a0e398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': {'label': 'NEGATIVE', 'score': 0.961249828338623}, 'llm_response': 'AI: You mentioned your name is Schnuffel.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_and_run_chain('Whats my name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat session started. Type 'exit' to quit.\n",
      "Therapist: How do you feel?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def main():\n",
    "    print(\"Chat session started. Type 'exit' to quit.\")\n",
    "\n",
    "    first_q = pick_random_question()\n",
    "    print(\"Therapist:\", first_q)\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if not user_input:\n",
    "            continue\n",
    "        if user_input.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"Session ended.\")\n",
    "            break\n",
    "\n",
    "        # Call your build_and_run_chain which uses ConversationBufferMemory internally\n",
    "        result = build_and_run_chain(user_input)\n",
    "        # Pretty-print the response\n",
    "        # e.g., show sentiment and the assistant reply\n",
    "        sentiment = result[\"sentiment\"]\n",
    "        assistant = result[\"llm_response\"]\n",
    "        print(f\"[Sentiment detected: {sentiment['label']} ({sentiment['score']:.2%})]\")\n",
    "        print(\"Therapist:\", assistant)\n",
    "\n",
    "    # Optionally, after exit, you can inspect full history:\n",
    "    # mem = memory.load_memory_variables({})\n",
    "    # print(\"Full chat history:\\n\", mem.get(\"history\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30154843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pocketcoach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
